{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c42ee9",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471688b1",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccd1d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Logistic Regression Clasificator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb2e04",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c1aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79502ed0",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efdc2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in data of Train and Data od Test\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b235660",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val  = train_test_split( x_train, y_train, test_size=0.166, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24928c66",
   "metadata": {},
   "source": [
    "### Information of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa076f2",
   "metadata": {},
   "source": [
    "The pixel values of the images range from __0__ through __255__. Scale these values to a range of __0__ to **1** by dividing the values by __255.0__. This also converts the sample data from integers to floating-point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc86b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val, x_test = x_train / 255.0, x_val/ 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d44dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (50040, 28, 28)\n",
      "y_train.shape : (50040,)\n",
      "\n",
      "x_val.shape : (9960, 28, 28)\n",
      "y_val.shape : (9960,)\n",
      "\n",
      "x_test.shape : (10000, 28, 28)\n",
      "y_test.shape : (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}\\n')\n",
    "\n",
    "print(f'x_val.shape : {x_val.shape}')\n",
    "print(f'y_val.shape : {y_val.shape}\\n')\n",
    "\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd28fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96ec5052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6dada",
   "metadata": {},
   "source": [
    "![](MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3787e1",
   "metadata": {},
   "source": [
    "# Build a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa041322",
   "metadata": {},
   "source": [
    "## Build a tf.keras.Sequential model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c8f85",
   "metadata": {},
   "source": [
    "![](1_N8UXaiUKWurFLdmEhEHiWg.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06c99a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: \"sequential_1\"\n",
    "# _________________________________________________________________\n",
    "#  Layer (type)                Output Shape              Param #   \n",
    "# =================================================================\n",
    "#  flatten_1 (Flatten)         (None, 784)               0         \n",
    "                                                                 \n",
    "#  dense_3 (Dense)             (None, 416)               326560    \n",
    "                                                                 \n",
    "#  dense_4 (Dense)             (None, 512)               213504    \n",
    "                                                                 \n",
    "#  dense_5 (Dense)             (None, 10)                5130   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df2d8af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 784)               615440    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 416)               326560    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 416)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               213504    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,160,634\n",
      "Trainable params: 1,160,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "# Input Layer\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  # Hidden Layer\n",
    "\n",
    "  tf.keras.layers.Dense(784, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "#   tf.keras.layers.BatchNormalization(),#regularización y reducir el sobreajuste del modelo\n",
    "  tf.keras.layers.Dropout(0.3),#regularización y reducir el sobreajuste del modelo\n",
    "\n",
    "  tf.keras.layers.Dense(416, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "#   tf.keras.layers.BatchNormalization(),#regularización y reducir el sobreajuste del modelo\n",
    "  tf.keras.layers.Dropout(0.3),#regularización y reducir el sobreajuste del modelo\n",
    "    \n",
    "  tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "#   tf.keras.layers.BatchNormalization(),#regularización y reducir el sobreajuste del modelo\n",
    "  tf.keras.layers.Dropout(0.3),#regularización y reducir el sobreajuste del modelo\n",
    "\n",
    " # Outoput Layer\n",
    "  tf.keras.layers.Dense(10,activation='softmax'), #Esta capa tiene 10 neuronas, clasificación de 10 categorías\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359bf74",
   "metadata": {},
   "source": [
    "### Detalles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e679d78",
   "metadata": {},
   "source": [
    "- __tf.keras.layers.Flatten__ (La capa de aplanamiento ) es una capa de procesamiento que convierte una matriz de __entrada multidimensional__ en un vector __unidimensional__. Esta capa se utiliza comúnmente como la primera capa en una red neuronal para transformar los datos de entrada. La capa de aplanamiento toma una matriz de entrada de __28x28__ (que representa una imagen de __28x28__ píxeles) y la convierte en un vector de __784__ elementos. Esto permite que la red neuronal procese la imagen como una __secuencia unidimensional__ de valores de píxeles en lugar de como una matriz bidimensional.<br><br>\n",
    "\n",
    "- __tf.keras.layers.Dense__ es una capa de redes neuronales __densamente conectada__ en TensorFlow, que implementa la operación de multiplicación de matriz y adición de sesgo en la entrada, seguida de una función de activación. Cada unidad o nodo en la capa está conectado a cada unidad en la capa anterior, y las entradas se multiplican por un conjunto de pesos y se suman a un sesgo antes de aplicar la función de activación. En otras palabras, la capa densa es una capa de neuronas completamente conectada, donde cada neurona en la capa anterior está conectada a todas las neuronas en la capa actual. La capa __Dense__ en la red neuronal se llama así porque __cada neurona de la capa está conectada a todas las neuronas de la capa anterior y de la capa siguiente__. Esto se conoce como __\"conexión densa\"__ o __\"totalmente conectada\"__. Los valores de entrada se multiplican por los pesos, se suman los resultados y se aplica una función de activación para producir los valores de salida de la capa.En resumen, la capa Dense es una capa de red neuronal en la que cada neurona está conectada a todas las neuronas de la capa anterior y de la capa siguiente, lo que permite aprender patrones complejos en los datos<br><br>\n",
    "\n",
    "- __tf.keras.layers.Dropout__ es una capa en TensorFlow que se utiliza para regularizar modelos de aprendizaje profundo y evitar el sobreajuste. Durante el entrenamiento, Dropout apaga de forma aleatoria un número predefinido de unidades de la capa anterior. Esto evita que las unidades se \"especialicen\" demasiado en un subconjunto particular de los datos de entrenamiento, lo que puede resultar en un modelo que no generaliza bien a datos nuevos. El porcentaje de unidades que se apaga se puede ajustar mediante un parámetro llamado \"rate\".<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da495618",
   "metadata": {},
   "source": [
    "Sequential is useful for stacking layers where each layer has one input tensor and one output tensor. Layers are functions with a known mathematical structure that can be reused and have trainable variables. Most TensorFlow models are composed of layers. This model uses the Flatten, Dense, and Dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59607f14",
   "metadata": {},
   "source": [
    "### predictions example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64f36f",
   "metadata": {},
   "source": [
    "For each example, the model returns a vector of __logits or log-odds scores__, one for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4893e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model(x_train[:1]).numpy()\n",
    "\n",
    "# for i, value in enumerate(predictions[0]):\n",
    "#     print(f'{i} : {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0787d",
   "metadata": {},
   "source": [
    "### converts these logits to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8091bf4",
   "metadata": {},
   "source": [
    "The __tf.nn.softmax__ function converts these logits to probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c19e7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693218b",
   "metadata": {},
   "source": [
    "## Define a loss function (Hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c7c55",
   "metadata": {},
   "source": [
    "Define a loss function for training using __losses.SparseCategoricalCrossentropy__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "610bd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632ee7c",
   "metadata": {},
   "source": [
    "The __loss function__ takes a __vector of ground truth values__ and a __vector of logits__ and __returns a scalar loss for each example__. This __loss__ is equal to the __negative log probability of the true class__: The __loss__ is __0__ if the model is sure of the __correct__ class.\n",
    "\n",
    "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to -tf.math.log(1/10) ~= 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc54a8",
   "metadata": {},
   "source": [
    "## Set the optimizer (Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "127a683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = loss_fn,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76eda96",
   "metadata": {},
   "source": [
    "### Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735ef51",
   "metadata": {},
   "source": [
    "Adam (Adaptive Moment Estimation) es un __algoritmo de optimización__ de __gradiente descendente estocástico__ utilizado comúnmente para entrenar redes neuronales en el aprendizaje profundo.\n",
    "\n",
    "Adam es un __método de optimización adaptativo__ que combina el enfoque de __Momentum__ y __RMSprop__. Al igual que RMSprop, Adam mantiene un promedio móvil de los cuadrados de los gradientes y, al igual que el enfoque de Momentum, mantiene un __promedio móvil de los gradientes__.\n",
    "\n",
    "El algoritmo adapta la tasa de aprendizaje de cada parámetro en función de la magnitud del gradiente y la magnitud de la media móvil de segundo orden de los gradientes. Adam también incluye un término de corrección de sesgo para compensar la inicialización sesgada de los promedios móviles en las primeras iteraciones.\n",
    "\n",
    "En resumen, Adam es un algoritmo de optimización de gradiente descendente estocástico que adapta la tasa de aprendizaje para cada parámetro en función de la magnitud del gradiente y la media móvil de segundo orden de los gradientes. Es muy popular en el aprendizaje profundo debido a su eficacia y eficiencia en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557d8ca",
   "metadata": {},
   "source": [
    "durante el el entrenamiento el leaning rate en adam varia?\n",
    "\n",
    "\n",
    "Sí, durante el entrenamiento, el optimizador __Adam__ puede __ajustar automáticamente la tasa de aprendizaje__. Adam utiliza un algoritmo adaptativo para calcular y actualizar la tasa de aprendizaje de forma dinámica en función de las estadísticas de los gradientes calculados durante el entrenamiento.\n",
    "\n",
    "__El algoritmo de Adam incluye dos componentes principales__: el __promedio móvil de primer orden (momentum)__ y __el promedio móvil de segundo orden (adaptive learning rate)__. Estos componentes permiten que Adam ajuste la tasa de aprendizaje para cada parámetro del modelo de forma individual, basándose en la magnitud de los gradientes y las características del historial de actualizaciones anteriores.\n",
    "\n",
    "A medida que el modelo se entrena y se actualizan los pesos, Adam adapta la tasa de aprendizaje de forma automática según la información acumulada de los gradientes previos. Esto puede ayudar a mejorar la eficiencia del entrenamiento y la capacidad del modelo para converger hacia una solución óptima.\n",
    "\n",
    "Es importante destacar que el algoritmo de Adam tiene valores predeterminados para otros hiperparámetros, como el momentum y el decaimiento de los promedios móviles. Estos hiperparámetros también pueden influir en la forma en que Adam ajusta la tasa de aprendizaje durante el entrenamiento.\n",
    "\n",
    "Si deseas tener un control más preciso sobre la tasa de aprendizaje y su variación durante el entrenamiento, puedes utilizar optimizadores personalizados o técnicas de programación de aprendizaje con tasas de aprendizaje variables. Sin embargo, en la mayoría de los casos, Adam es una opción sólida y eficiente que ajusta automáticamente la tasa de aprendizaje de manera efectiva para la mayoría de los problemas de aprendizaje profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d1fc7",
   "metadata": {},
   "source": [
    "# Train and evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ba162",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bbc35",
   "metadata": {},
   "source": [
    "Use the __Model.fit__ Method to adjust your model parameters and minimize the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58bc63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train,  \n",
    "    epochs=n_epoch, \n",
    "    verbose=1, \n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc14682",
   "metadata": {},
   "source": [
    "# Grafics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "571ac14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizacion_resultados(n,history):\n",
    "    epochs = [i for i in range( n  )]\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    train_acc = history.history[\"accuracy\"]\n",
    "    train_loss = history.history[\"loss\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    fig.set_size_inches(16,9)\n",
    "\n",
    "    ax[0].plot(epochs, train_acc, \"go-\",label = \"Entrenamiento accuracy\")\n",
    "    ax[0].plot(epochs, val_acc, \"bo-\",label = \"Validación accuracy\")\n",
    "    ax[0].set_title(\"Entrenamiento y validación accuracy\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "    ax[0].grid()\n",
    "\n",
    "    ax[1].plot(epochs, train_loss, \"go-\",label = \"Entrenamiento loss\")\n",
    "    ax[1].plot(epochs, val_loss, \"bo-\",label = \"Validación loss\")\n",
    "    ax[1].set_title(\"Entrenamiento y validación loss\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Loss\")\n",
    "    ax[1].grid()\n",
    "    \n",
    "\n",
    "    plt.show() \n",
    "visualizacion_resultados(n_epoch,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e289df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ab9b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1558d7e",
   "metadata": {},
   "source": [
    "# evaluate for examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67efa3",
   "metadata": {},
   "source": [
    "The __Model.evaluate__ method checks the model's performance, usually on a validation set or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "982beb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63b762cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_test[:1]).numpy()\n",
    "\n",
    "for i, value in enumerate(predictions[0]):\n",
    "    print(f'{i} : {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50fb6a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xaux=x_test\n",
    "yauc=y_test\n",
    "for n in range(0,10)  :\n",
    "    predictions = model(xaux[n:n+1]).numpy()\n",
    "\n",
    "    arr = tf.nn.softmax(predictions).numpy()[0]\n",
    "\n",
    "    # Convertimos el array a un diccionario\n",
    "    dict_arr = dict(zip(range(len(arr)), arr.tolist()))\n",
    "\n",
    "    d_ordenado = dict(sorted(dict_arr.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    for key, value in d_ordenado.items():\n",
    "        print( \"El valor real  [\", yauc[n], '] predicho :[',key, \"] con\", value*100,'probabilidad','\\n')\n",
    "    print('<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52cda585",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50) : \n",
    "    print(i,y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d182850",
   "metadata": {},
   "source": [
    "# Keras tuners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "396ae74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para eliminar los modelos que hayan quedado guardados en memoria\n",
    "tf.keras.backend.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25e8ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a756f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (50040, 28, 28)\n",
      "y_train.shape : (50040,)\n",
      "\n",
      "x_val.shape : (9960, 28, 28)\n",
      "y_val.shape : (9960,)\n",
      "\n",
      "x_test.shape : (10000, 28, 28)\n",
      "y_test.shape : (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}\\n')\n",
    "\n",
    "print(f'x_val.shape : {x_val.shape}')\n",
    "print(f'y_val.shape : {y_val.shape}\\n')\n",
    "\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03c4dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de modelo para Keras Tuner\n",
    "# hp: es el objeto hp que se utiliza para ajustar los hiperparámetros del modelo.\n",
    "def build_model(hp):\n",
    "    \n",
    "    #se crea un modelo secuencial utilizando la biblioteca Keras.\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Esta capa transforma los datos de entrada en un vector unidimensional.\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    #se itera sobre un rango determinado por el valor de hiperparámetro num_layers. \n",
    "    #Este hiperparámetro define el número de capas ocultas en el modelo.\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=5)):\n",
    "        \n",
    "        # se agregan capas densas (fully connected) al modelo. \n",
    "        # El número de unidades en cada capa se ajusta utilizando el \n",
    "        # hiperparámetro units con un rango específico definido por min_value, max_value y step.\n",
    "        model.add( layers.Dense( units      = hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n",
    "                                 activation = 'relu') )\n",
    "\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8700a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el optimizador de búsqueda aleatoria\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=3,\n",
    "    factor=3,\n",
    "    project_name='mnist_tuner',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad20a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 01m 15s]\n",
      "val_accuracy: 0.975301206111908\n",
      "\n",
      "Best val_accuracy So Far: 0.975301206111908\n",
      "Total elapsed time: 00h 04m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Realizar la búsqueda de hiperparámetros\n",
    "tuner.search(x_train, y_train,\n",
    "             epochs=10,\n",
    "             validation_data=(x_val, y_val))\n",
    "\n",
    "# Obtener la mejor combinación de hiperparámetros\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e72f9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir el modelo con la mejor combinación de hiperparámetros\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de9d052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1564/1564 [==============================] - 25s 13ms/step - loss: 0.2013 - accuracy: 0.9394 - val_loss: 0.1085 - val_accuracy: 0.9684\n",
      "Epoch 2/5\n",
      "1564/1564 [==============================] - 18s 12ms/step - loss: 0.0880 - accuracy: 0.9728 - val_loss: 0.0873 - val_accuracy: 0.9733\n",
      "Epoch 3/5\n",
      "1564/1564 [==============================] - 19s 12ms/step - loss: 0.0614 - accuracy: 0.9805 - val_loss: 0.0869 - val_accuracy: 0.9745\n",
      "Epoch 4/5\n",
      "1564/1564 [==============================] - 18s 12ms/step - loss: 0.0446 - accuracy: 0.9860 - val_loss: 0.0845 - val_accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1564/1564 [==============================] - 20s 12ms/step - loss: 0.0359 - accuracy: 0.9887 - val_loss: 0.0847 - val_accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b0cb023d90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo con la mejor combinación de hiperparámetros\n",
    "best_model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab6c44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 416)               326560    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               213504    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 545,194\n",
      "Trainable params: 545,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
